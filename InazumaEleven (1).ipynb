{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou9VWRkaUeca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Chargement des fichiers CSV\n",
        "waiting_times_train = pd.read_csv('waiting_times_train.csv')\n",
        "weather_data = pd.read_csv('weather_data.csv')\n",
        "waiting_times_X_test_val = pd.read_csv('waiting_times_X_test_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition de la fonction pour ajouter les caractéristiques temporelles\n",
        "def add_time_features(df):\n",
        "    df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
        "    df['DAY_OF_WEEK'] = df['DATETIME'].dt.dayofweek\n",
        "    df['MONTH'] = df['DATETIME'].dt.month\n",
        "    df['HOUR'] = df['DATETIME'].dt.hour\n",
        "    df['IS_WEEKEND'] = df['DAY_OF_WEEK'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "    # Ajouter le numéro de semaine\n",
        "    df['WEEK_OF_YEAR'] = df['DATETIME'].dt.isocalendar().week\n",
        "\n",
        "    # Définir les saisons\n",
        "    def get_season(month):\n",
        "        if month in [3, 4, 5]:\n",
        "            return 0\n",
        "        elif month in [6, 7, 8]:\n",
        "            return 1\n",
        "        elif month in [9, 10, 11]:\n",
        "            return 2\n",
        "        else:\n",
        "            return 3\n",
        "\n",
        "    df['SEASON'] = df['MONTH'].apply(get_season)\n",
        "\n",
        "    df['DAY_OF_YEAR'] = df['DATETIME'].dt.dayofyear\n",
        "    df['QUARTER_HOUR'] = df['DATETIME'].dt.hour * 4 + df['DATETIME'].dt.minute // 15\n",
        "\n",
        "    def get_period_of_day(hour):\n",
        "        if 8 <= hour < 11:\n",
        "            return 0\n",
        "        elif 11 <= hour < 14:\n",
        "            return 1\n",
        "        elif 14 <= hour < 18:\n",
        "            return 2\n",
        "        else:\n",
        "            return 3\n",
        "\n",
        "    df['PERIOD_OF_DAY'] = df['DATETIME'].dt.hour.apply(get_period_of_day)\n",
        "\n",
        "    df['HOUR_SIN'] = np.sin(2 * np.pi * df['DATETIME'].dt.hour / 24)\n",
        "\n",
        "    df['WEEKEND_NUMBER'] = 0  # Initialiser avec 0 pour tous les jours\n",
        "    weekend_mask = df['IS_WEEKEND'] == 1  # Masque pour identifier les week-ends\n",
        "    df.loc[weekend_mask, 'WEEKEND_NUMBER'] = df.loc[weekend_mask, 'DATETIME'].dt.isocalendar().week\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "# Assurez-vous d'appliquer cette fonction seulement aux DataFrames nécessitant ces colonnes\n",
        "waiting_times_train = add_time_features(waiting_times_train)\n",
        "waiting_times_X_test_val = add_time_features(waiting_times_X_test_val)\n",
        "weather_data = add_time_features(weather_data)\n",
        "\n",
        "\n",
        "train_data = pd.merge(waiting_times_train, weather_data, on='DATETIME', how='left', suffixes=('', '_drop'))\n",
        "test_data = pd.merge(waiting_times_X_test_val, weather_data, on='DATETIME', how='left', suffixes=('', '_drop'))\n",
        "\n",
        "# Supprimer les colonnes avec le suffixe _drop\n",
        "train_data = train_data[[c for c in train_data.columns if not c.endswith('_drop')]]\n",
        "test_data = test_data[[c for c in test_data.columns if not c.endswith('_drop')]]\n",
        "\n",
        "\n",
        "X_train = train_data[['ADJUST_CAPACITY', 'HOUR_SIN', 'PERIOD_OF_DAY', 'QUARTER_HOUR', 'DAY_OF_YEAR', 'WEEK_OF_YEAR', 'WEEKEND_NUMBER', 'SEASON', 'DOWNTIME', 'TIME_TO_PARADE_1', 'TIME_TO_PARADE_2', 'CURRENT_WAIT_TIME', 'TIME_TO_NIGHT_SHOW', 'temp', 'dew_point', 'feels_like', 'pressure', 'humidity', 'wind_speed', 'rain_1h', 'snow_1h', 'clouds_all', 'DAY_OF_WEEK', 'MONTH', 'IS_WEEKEND', 'HOUR']].fillna(0)\n",
        "y_train = train_data['WAIT_TIME_IN_2H']\n",
        "\n",
        "X_test = test_data[['ADJUST_CAPACITY', 'HOUR_SIN', 'PERIOD_OF_DAY', 'QUARTER_HOUR', 'DAY_OF_YEAR', 'WEEK_OF_YEAR', 'WEEKEND_NUMBER', 'SEASON', 'DOWNTIME', 'TIME_TO_PARADE_1', 'TIME_TO_PARADE_2', 'CURRENT_WAIT_TIME', 'TIME_TO_NIGHT_SHOW', 'temp', 'dew_point', 'feels_like', 'pressure', 'humidity', 'wind_speed', 'rain_1h', 'snow_1h', 'clouds_all', 'DAY_OF_WEEK', 'MONTH', 'IS_WEEKEND', 'HOUR']].fillna(0)"
      ],
      "metadata": {
        "id": "kt--4n2PWRTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the pipeline steps\n",
        "steps = [\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # optimal\n",
        "    ('scaler', MinMaxScaler()), # optimal\n",
        "    ('model', XGBRegressor(n_estimators=1000, # optimal\n",
        "                           random_state=30, # optimal\n",
        "                           learning_rate=0.025,   # optimal\n",
        "                           max_depth=8,         # optimal\n",
        "                           subsample=0.95,       # optimal\n",
        "                           colsample_bytree=0.56, # optimal\n",
        "                           gamma=1,           # optimal\n",
        "                           reg_alpha=200,       # optimal\n",
        "                           reg_lambda=10,       # optimal\n",
        "                           min_child_weight=150,  # optimal\n",
        "                           ))\n",
        "]\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Création d'un DataFrame pour les prédictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'DATETIME': test_data['DATETIME'],\n",
        "    'ENTITY_DESCRIPTION_SHORT': test_data['ENTITY_DESCRIPTION_SHORT'],\n",
        "    'y_pred': y_pred,\n",
        "    'KEY': 'c57d53a31f68e864e929524b80c3dfe31190a5e431187fa12f'  # Utilisez 'Validation' pour le test de validation, changez selon le besoin pour le test final\n",
        "})\n",
        "\n",
        "predictions_df = predictions_df[['DATETIME', 'ENTITY_DESCRIPTION_SHORT', 'y_pred', 'KEY']]\n",
        "predictions_df.to_csv('predictions_final.csv', index=False)"
      ],
      "metadata": {
        "id": "G7CIs6FNg5q-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}